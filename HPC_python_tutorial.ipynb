{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca099052-6630-4cd0-9bf1-1d3a67783721",
   "metadata": {},
   "source": [
    "# Tutorial on speeding up python code on a computing cluster\n",
    "\n",
    "* Machine learning jobs can take a long time to run\n",
    "* How can your code exploit the number of cores\n",
    "\n",
    "There are a number of motivations for running machine learning  or AI jobs in parallel on a computer cluster.\n",
    "When a python script is run on a worker node on the cluster, by default, it runs on only one core. Some of the nodes on the cluster have 16 cores. If the script could use all 16 cores then potentially the script could run 16 times faster. In practice the speedup is less than the number of cores. Larger data sets may not fit into the memory of a single node, so if the python script can run on multiple nodes larger data sets could be used in the analysis.\n",
    "\n",
    "In this tutorial we will look at some basic tools to use more of the cores on the cluster with an emphasis on data science and AI applications. Most applications will be Embarrassingly parallel (https://en.wikipedia.org/wiki/Embarrassingly_parallel) where there is little communication between the python running on different cores until the jobs end and the results are collected. \n",
    "\n",
    "## Background on parallel computer.\n",
    "\n",
    "A high performance computer is made of compute nodes connected by a network. Each compute node has a number of cores.\n",
    "\n",
    "![Cores and nodes](https://www.researchgate.net/publication/323058591/figure/fig1/AS:622777811890177@1525493209635/A-modern-NUMA-system-with-four-nodes-and-eight-cores-per-node.png)\n",
    "\n",
    "There are different ways of running programs on parallel computers made from nodes connected by a network\n",
    "* **Threads**\n",
    "* **Processes**\n",
    "* **MPI** Message Passing Interface\n",
    "* **shared-memory parallel programming**\n",
    "\n",
    "\n",
    "\n",
    "## Running the examples on the cluster\n",
    "\n",
    "This file is a jupyter notebook that you can download to your own machine.\n",
    "I suggest you paste the python code into a text file to run on the cluster \n",
    "\n",
    "This basic tutorial was created by Craig McNeile  https://www.plymouth.ac.uk/staff/craig-mcneile by modifying examples from other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f6453-2769-4437-b43c-f346146e3c4d",
   "metadata": {},
   "source": [
    "### Philosophy \n",
    "\n",
    "As the python is run on more cores then the time taken should decrease. To test this the code should be run with a different number of cores and the execution time should be measured. The code below shows how to time how long a function takes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab18d48-00d6-443f-9b26-7304a09f2404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task...\n",
      "Time taken to run the function 0.43 s\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from time import time\n",
    " \n",
    "# define a cpu-intensive task\n",
    "def task(arg):\n",
    "    return sum([math.sqrt(i) for i in range(1, arg)])\n",
    " \n",
    "\n",
    "# report a message\n",
    "print('Starting task...')\n",
    "start = time()\n",
    "# perform calculations\n",
    "results = task(5000000)\n",
    "time_taken = time() - start\n",
    "print(f\"Time taken to run the function {time_taken:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3340d722-b470-4a3a-b2a6-ef8d91e33861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0554012659995351, 0.05171712700393982, 0.05300903299939819, 0.04790831301943399, 0.04787989699980244]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import_module = \"import math\"\n",
    "testcode = ''' \n",
    "def task(): \n",
    "    return sum([math.sqrt(i) for i in range(1,5000000 )])\n",
    "'''\n",
    "print(timeit.repeat(stmt=testcode, setup=import_module))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c9fe8-d5a1-4f83-bd0d-2b01f902c58d",
   "metadata": {},
   "source": [
    "### Tip\n",
    "\n",
    "If you want an explanation of what some of the python code is doing you can write\n",
    "**Explain the python code** and paste the code into ChatGPT https://chat.openai.com/\n",
    "See this example below\n",
    "\n",
    "<img src=\"https://github.com/cmcneile/HPC-tutorial/blob/main/ChatGPt.png?raw=true\" alt=\"ChatGPT example\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b9b95-f09b-4ab2-9de2-8cca83a01bdf",
   "metadata": {},
   "source": [
    "## Plan of the tutorial\n",
    "\n",
    "* Running machine learning algorithms in the python scikit-learn module on multiple cores\n",
    "* Using the numba library (https://numba.pydata.org/) to run code on multiple cores.\n",
    "* Running python on multiple processes that run on multiple cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fff98-7ce3-4d4b-921d-ff42a2d6864f",
   "metadata": {},
   "source": [
    "##  Using multiple cores to run machine learning jobs\n",
    "\n",
    "*  Example 1 and 2 were developed from this page  https://machinelearningmastery.com/multi-core-machine-learning-in-python/\n",
    "*  See the reference material on scikit learn about running paralllel jobs https://scikit-learn.org/stable/computing/parallelism.html\n",
    "\n",
    "Sometimes the hard work of making an application run on multiple cores has been done by the team that wrote the application. The user just has to input the number of cores to use. Sometimes there are other options that can change the speed of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076c4f3b-a540-4f0e-bce1-482ed0c58d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmcneile/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396782e-6b26-4ead-b675-5f52e52c7b53",
   "metadata": {},
   "source": [
    "* We create a fake data set that the classifier can be run on.\n",
    "* The data set is then split into a test (20%) and training (80%) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc81ea3f-4f1c-41b0-9c7b-d9efeaf661fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "\n",
    "# break the dataset into test and train data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc8a08-b374-410c-a063-94d25bf744d2",
   "metadata": {},
   "source": [
    "We createa a classifier using the random forest method (https://www.ibm.com/topics/random-forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb35a1e3-7e27-40b7-8db3-211a5fb5ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9595\n",
      "The confusion matrix\n",
      "[[966  48]\n",
      " [ 33 953]]\n",
      "Time taken 3.365 seconds with  4 cores\n"
     ]
    }
   ],
   "source": [
    "number_of_cores = 1\n",
    "# define the model\n",
    "model = RandomForestClassifier(n_estimators=500, n_jobs=number_of_cores)\n",
    "# record current time\n",
    "start = time()\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# record current time\n",
    "end = time()\n",
    "# report execution time\n",
    "\n",
    "# evaluate the model on the test data set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"The confusion matrix\")\n",
    "print(cm)\n",
    "\n",
    "result = end - start\n",
    "print('Time taken %.3f seconds' % result, \"with \", number_of_cores, \"cores\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138288f-da0f-428e-b046-41d8c3360462",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Try running the above code with number_of_cores = 1,2,4,8,16,32 and record the timings. How does the time taken change?\n",
    "* Is the model.predict function call run on multiple cores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25ac55-104e-47b6-9eea-b9a735a85ae2",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "An important part of the machine learning is tuning the hyperparamters to obtain good performance of the algorithm.\n",
    "The hyperparameters can be looped over a grid of the different possibilities or random points can be selected in a region of the space of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1656325c-67cc-4fa4-be51-3fd3f7c14663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from time import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7e2422f-4e0d-440a-948f-0e8a28dbf370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset again\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "\n",
    "# break the dataset into test and train data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9fc6390-6804-4891-a19f-3c358aff741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the random search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmcneile/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/cmcneile/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/cmcneile/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/cmcneile/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cores =  4 Time taken 26.115 seconds\n"
     ]
    }
   ],
   "source": [
    "param_dist = {'n_estimators': randint(50,500),\n",
    "              'max_depth': randint(1,20)}\n",
    "\n",
    "# Create a random forest classifier\n",
    "modelA = RandomForestClassifier()\n",
    "\n",
    "njob = 4\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(modelA, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5, n_jobs=njob)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "\n",
    "print(\"Starting the random search\")\n",
    "\n",
    "start = time()\n",
    "rand_search.fit(X_train, y_train)\n",
    "end = time()\n",
    "\n",
    "result = end - start\n",
    "print(\"No cores = \", njob , 'Time taken %.3f seconds' % result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06036814-c10b-40f3-88a2-5b1d3b92bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9475\n",
      "[[911  74]\n",
      " [ 31 984]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred = rand_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "##ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07146710-25d9-4748-b073-7416cce131bf",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "*  Try running the above code with number_of_cores = 1,2,4,8,16,32 . How does the time taken change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d76fa-51b3-46ae-a1f7-f0deeea71a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "688b3bbb-39ea-4800-b3e2-2d7c9e0f2fa6",
   "metadata": {},
   "source": [
    "### Example 3 \n",
    "\n",
    "* Some python code can be speeded by using libraries\n",
    "* One such library is https://numba.pydata.org/\n",
    "* Numba has options to run code in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28bca89e-2d7c-4a61-96f7-ef5912c4697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.  12.  13.  14.  15.  16.  17.  18.]\n",
      " [ 19.  20.  21.  22.  23.  24.  25.  26.  27.  28.]\n",
      " [ 29.  30.  31.  32.  33.  34.  35.  36.  37.  38.]\n",
      " [ 39.  40.  41.  42.  43.  44.  45.  46.  47.  48.]\n",
      " [ 49.  50.  51.  52.  53.  54.  55.  56.  57.  58.]\n",
      " [ 59.  60.  61.  62.  63.  64.  65.  66.  67.  68.]\n",
      " [ 69.  70.  71.  72.  73.  74.  75.  76.  77.  78.]\n",
      " [ 79.  80.  81.  82.  83.  84.  85.  86.  87.  88.]\n",
      " [ 89.  90.  91.  92.  93.  94.  95.  96.  97.  98.]\n",
      " [ 99. 100. 101. 102. 103. 104. 105. 106. 107. 108.]]\n",
      "Time taken 0.001 seconds\n",
      "Time taken 0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "#@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "\n",
    "#@jit(parallel = True)\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "start = time()\n",
    "print(go_fast(x))\n",
    "end = time()\n",
    "\n",
    "result = end - start\n",
    "print('Time taken %.3f seconds' % result)\n",
    "\n",
    "start = time()\n",
    "ans = go_fast(x)\n",
    "end = time()\n",
    "print('Time taken %.3f seconds' % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a686e6-a136-467c-bc58-e7d2c7dca0fe",
   "metadata": {},
   "source": [
    "### Example 4 \n",
    "\n",
    "*  We can use  concurrent.futures module (https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures) to run functions over multiple cores.\n",
    "*  This example is based on this page: https://superfastpython.com/python-use-all-cpu-cores/\n",
    "\n",
    "In the example below the **task** function is run on multiple cores. \n",
    "\n",
    "The task function computes the sum of the square root of each integer in a list\n",
    "\n",
    "$$\n",
    "task = \\sum_{i=1}^{i=arg}  \\sqrt{i}\n",
    "$$\n",
    "\n",
    "In your own application you would modify the function **task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f3628a-6843-43c3-89ab-e3b7c9752a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task...\n",
      "Time taken 86.125 seconds\n"
     ]
    }
   ],
   "source": [
    "# SuperFastPython.com\n",
    "# example of a program that does not use all cpu cores\n",
    "import math\n",
    "from time import time\n",
    " \n",
    "# define a cpu-intensive task\n",
    "def task(arg):\n",
    "    return sum([math.sqrt(i) for i in range(1, arg)])\n",
    " \n",
    "\n",
    "# report a message\n",
    "print('Starting task...')\n",
    "start = time()\n",
    "# perform calculations\n",
    "results = [task(i) for i in range(1,50000)]\n",
    "end = time()\n",
    "\n",
    "result = end - start\n",
    "print('Time taken %.3f seconds' % result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027be693-3ccd-41cc-ae3d-ea249329ca86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 33.307 seconds on  4 cores\n"
     ]
    }
   ],
   "source": [
    "number_of_cores = 2\n",
    "# create the process pool\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "start = time()\n",
    "with  ProcessPoolExecutor(number_of_cores) as exe:\n",
    "    # perform calculations\n",
    "    results = exe.map(task, range(1,50000))\n",
    "\n",
    "end = time()\n",
    "\n",
    "result = end - start\n",
    "print('Time taken %.3f seconds' % result, \"on \" , number_of_cores , \"cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72083c1-1039-43ac-b35a-150946bf75b4",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "*  Try running the above code with number_of_cores = 1,2,4,8,16,32 . How does the time taken change?\n",
    "*  Repeat the timing measurements with the function **ThreadPoolExecutor**\n",
    "*  What is the difference between threads and processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474aa36-b38b-4a79-8f0d-9ba74c3213f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
